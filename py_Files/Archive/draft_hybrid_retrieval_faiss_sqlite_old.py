# -*- coding: utf-8 -*-
"""draft_Hybrid_Retrieval_FAISS_SQLite.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vifLCuo3YY0eD3jtS_DTBzahxRdJdx2J
"""

#Hybrid Retrieval SQL and FAISS (NOT TESTED)

# 0. Install dependencies (run once)
!pip install -q faiss-cpu sentence-transformers

# 1. Imports and DB initialization
import re
import sqlite3
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

def initialize_db(db_path="script_metadata.db"):
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS script_lines (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            season INTEGER,
            episode INTEGER,
            scene TEXT,
            speaker TEXT,
            line_number INTEGER,
            actions TEXT,
            clean_text TEXT,
            raw_line TEXT
        )
    """)
    conn.commit()
    return conn, cur

# 2. Parse the script
def parse_script_lines(raw_lines):
    scene_pattern = re.compile(r"^\[(.*)\]$")
    speaker_pattern = re.compile(r"^([A-Za-z0-9 ']+):")
    action_pattern = re.compile(r"\((.*?)\)")

    parsed = []
    scene = None
    season = None
    episode = None

    for line_number, raw_line in enumerate(raw_lines):
        line = raw_line.strip()
        if not line:
            continue

        if line.upper().startswith("SEASON"):
            try:
                season = int(line.split()[1])
            except:
                season = None
            continue
        if line.upper().startswith("EPISODE"):
            try:
                episode = int(line.split()[1])
            except:
                episode = None
            continue

        scene_match = scene_pattern.match(line)
        if scene_match:
            scene = scene_match.group(1).strip()
            continue

        speaker_match = speaker_pattern.match(line)
        if speaker_match:
            speaker = speaker_match.group(1).strip()
            dialogue_with_actions = line[len(speaker)+1:].strip()
        else:
            speaker = None
            dialogue_with_actions = line

        actions = action_pattern.findall(dialogue_with_actions)
        clean_text = action_pattern.sub("", dialogue_with_actions).strip()

        parsed.append({
            "season": season,
            "episode": episode,
            "scene": scene,
            "speaker": speaker,
            "line_number": line_number,
            "actions": actions,
            "clean_text": clean_text if clean_text else None,
            "raw_line": raw_line.strip()
        })

    return parsed

# 3. Insert metadata into SQLite
def insert_line(cur, line_data):
    cur.execute("""
        INSERT INTO script_lines (
            season, episode, scene, speaker, line_number, actions, clean_text, raw_line
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        line_data['season'],
        line_data['episode'],
        line_data['scene'],
        line_data['speaker'],
        line_data['line_number'],
        ','.join(line_data['actions']),
        line_data['clean_text'],
        line_data['raw_line']
    ))

# 4. Build FAISS index
def build_faiss_index(texts, model):
    embeddings = model.encode(texts, convert_to_numpy=True)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index, embeddings

# 5. Semantic search that retrieves metadata from SQLite
def semantic_search(query, faiss_index, model, cur, k=5):
    q_emb = model.encode([query], convert_to_numpy=True)
    D, I = faiss_index.search(q_emb, k)
    results = []
    for idx in I[0]:
        # SQLite ids start at 1, FAISS indices at 0
        cur.execute("SELECT season, episode, scene, speaker, line_number, actions, clean_text FROM script_lines WHERE id = ?", (idx+1,))
        row = cur.fetchone()
        if row:
            results.append({
                "season": row[0],
                "episode": row[1],
                "scene": row[2],
                "speaker": row[3],
                "line_number": row[4],
                "actions": row[5].split(",") if row[5] else [],
                "clean_text": row[6]
            })
    return results

# ========== Run the pipeline ==========

# Load raw lines from your script file
file_path = "/content/drive/MyDrive/IR_Project/derry_girls_script/DERRY-GIRLS-SCRIPT.txt"  # Adjust if needed
with open(file_path, "r", encoding="windows-1252") as f:
    raw_lines = f.readlines()

parsed_lines = parse_script_lines(raw_lines)
print(f"Parsed {len(parsed_lines)} lines with metadata.")

conn, cur = initialize_db()

print("Inserting parsed lines into SQLite...")
for line_data in parsed_lines:
    insert_line(cur, line_data)
conn.commit()

# Prepare texts for embeddings (use clean_text or empty string)
texts = [line['clean_text'] or "" for line in parsed_lines]

print("Loading sentence transformer model...")
model = SentenceTransformer("all-MiniLM-L6-v2")

print("Building FAISS index...")
faiss_index, embeddings = build_faiss_index(texts, model)
print(f"FAISS index contains {faiss_index.ntotal} vectors.")

# Example semantic search
query = "Protestants and Catholics arguments"
print(f"\nSemantic search for: {query}")
results = semantic_search(query, faiss_index, model, cur, k=5)

for i, res in enumerate(results, 1):
    print(f"\nResult #{i}:")
    print(f" Season {res['season']}, Episode {res['episode']}, Scene: {res['scene']}")
    print(f" Speaker: {res['speaker']}, Actions: {res['actions']}")
    print(f" Text: {res['clean_text']}")